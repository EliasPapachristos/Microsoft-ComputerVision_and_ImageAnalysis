{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Module 4, Lab 1: Using ResNet with Fully Convolutional Layers to Implement Semantic Segmentation\n\nIn this tutorial, we will add fully-convolutional layers to the ResNet model, allowing us to use it to perform semantic segmentation. In other words, by adding additional layers to our existing object classifier network, we are extending its capability to be able to predict the class of each pixel in the input image.\n\nFor this task, we have again chosen ResNet20 as our base model.  ResNet is configurable in terms of the number of layers, with \nResNet20 being relatively shallow.  This allows us to perform training in a reasonable amount of time, whilst still \ndemonstrating the important concepts and abilities of this architecture. It is relatively easy to increase the depth of the\nnetwork to one of the larger configurations, and this is included as an optional exercise for the student.\n\nIn this tutorial, although we have added layer to ResNet to create a new model, we will train this model from scratch. In the next tutorial, we will introduce and use the concept of *Transfer Learning*, where pre-existing learned knowledge is used to inform our approach and improve our results.\n\nAnywhere you see `*** YOUR CODE HERE ***`, you will be expected to write some simple Python (following the instructions in the text) to complete a function.  Hint: if you get stuck, keep reading through the lab.\n\nLet's get started!\n\n\n# Setup Code\n\nFirst, as usual, we need some boilerplate setup code to load the Microsoft Cognitive Toolkit (previously known as the CNTK) libraries etc."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "All our code examples require the Cognitive Toolkit version 2.4. We will attempt to check if we have the Cognitive Toolkit version 2.4 locally. If not, we'll have to use the Python package manager `pip` to install it. If that is needed, that could take a few minutes."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Copyright (c) Microsoft. All rights reserved.\n#\n# Licensed under the MIT license. See LICENSE.md file in the project root\n# for full license information.\n# ==============================================================================\n\n# For Azure Notebooks, we will update Microsoft Cognitive Toolkit version to 2.4 \n# you can comment out the following line if you are running in your own local Jupyter Notebook setup and already have\n# CNTK 2.4 installed\n!pip install --upgrade --no-deps https://cntk.ai/PythonWheel/CPU-Only/cntk-2.4-cp35-cp35m-linux_x86_64.whl\n\nimport cntk as C\nprint (\"Using Microsoft Cognitive Toolkit version {}\".format(C.__version__))\n\nimport numpy as np\nprint (\"Using numpy version {}\".format(np.__version__))",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Collecting cntk==2.4 from https://cntk.ai/PythonWheel/CPU-Only/cntk-2.4-cp35-cp35m-linux_x86_64.whl\n  Using cached https://cntk.ai/PythonWheel/CPU-Only/cntk-2.4-cp35-cp35m-linux_x86_64.whl\nInstalling collected packages: cntk\n  Found existing installation: cntk 2.4\n    Uninstalling cntk-2.4:\n      Successfully uninstalled cntk-2.4\nSuccessfully installed cntk-2.4\nUsing Microsoft Cognitive Toolkit version 2.4\nUsing numpy version 1.15.4\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import matplotlib.pyplot as plt\n%matplotlib inline\nimport time\nimport sys\nimport os\nimport cv2\nfrom IPython.display import Image, display #, SVG\nfrom cntk.learners import learning_rate_schedule, UnitType\nfrom cntk.initializer import he_normal\nfrom cntk.layers import AveragePooling, BatchNormalization, Convolution, Dense\nfrom cntk.ops import element_times, relu, sigmoid\nfrom cntk import load_model, placeholder, Constant\nfrom tqdm import *          # tqdm allows us to pretty-print our loops later on...\nimport helper # some functions to plot images\nimport coco   # local class to read the COCO images and labels\nimport gc # Python garbage collector\n\n# Paths relative to current python file.\nabs_path  = os.path.dirname(os.path.abspath('.'))\ndata_path = os.path.join(abs_path, \"../../data/M4\")\nzip_path = os.path.join(abs_path, \"../data-zip\")\noutput_path = os.path.join(abs_path, \"lab1/output\")\n\n",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now, let's setup and call our function to setup the large dataset that is  needed for this lab. \n\nWhy are we doing this? We need this step because Azure Notebooks has two storage areas: a persistent but slow area, and a transient but fast area. To ensure that this lab is able to execute as fast as possible on Azure Notebooks, we unzip our data set into this transient area once per session.  If it has already been unzipped then we'll automatically detect this and skip the step, so it is safe to run this next code block at any time.\n\nDepending on how you are executing the lab, this step can take a minute or two.  The output you should expect to see from the following code block will look something like:\n\n    Start unzipping data files in /home/nbuser/library/Module4/../data-zip to /home/nbuser/library/Module4/../../data/M4\n    zip file count:1\n    Destination folder /home/nbuser/library/Module4/../../data/M4 doesn't exist, creating it\n    Start unzipping files to /home/nbuser/library/Module4/../../data/M4\n       unzipping CocoMS.zip ...\n    Complete: Files have been unzipped to /home/nbuser/library/Module4/../../data/M4"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import sys\nimport os\nimport zipfile\nimport fnmatch\n\ndef hydrate(zip_path, dest_path):\n    print(\"Start unzipping data files in {0} to {1}\".format(zip_path,dest_path))\n    if (os.path.exists(zip_path) == False):\n        print(\"The source folder {0} doesn't exist, so quitting\".format(zip_path))\n        quit()\n\n    zipfile_count = len(fnmatch.filter(os.listdir(zip_path), '*.zip'))\n    if (zipfile_count == 0):\n        print(\"No zip (.zip) files in {0}, so quitting \".format(zip_path))\n\n    print(\"zip file count:%s\" % zipfile_count)\n\n    if (os.path.exists(dest_path) == False):\n        print(\"Destination folder {0} doesn't exist, creating it\".format(dest_path))\n        os.makedirs(dest_path)\n\n        # Extract all zip files from zip_path to dest_path\n        print(\"Start unzipping files to {0}\".format(dest_path))\n        for item in os.listdir(zip_path): # loop through items in dir\n            if item.endswith(\".zip\"): # check for \".zip\" extension\n                print(\"   unzipping {0} ...\".format(item))\n                file_name = os.path.join(zip_path,item) # get full path of files\n                zip_ref = zipfile.ZipFile(file_name) # create zipfile object\n                zip_ref.extractall(dest_path) # extract file to dir\n                zip_ref.close() # close file\n    else:\n        print(\"data folder already populated\")\n\n    print(\"Complete: Files have been unzipped to {0}\".format(dest_path))\n    \nhydrate(zip_path, data_path)",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Start unzipping data files in /home/nbuser/library/Module4/../data-zip to /home/nbuser/library/Module4/../../data/M4\nzip file count:6\ndata folder already populated\nComplete: Files have been unzipped to /home/nbuser/library/Module4/../../data/M4\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "In the next code block, we check to see whether the Microsoft Cognitive Toolkit can use a GPU for its work. We are going to use the best available device (GPU, if available, else CPU)."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "try:\n    isUsingGPU # if this is our first time running, this will cause an exception as undefined\nexcept NameError:\n    try:\n        isUsingGPU = C.device.try_set_default_device(C.device.gpu(0))\n    except ValueError:\n        isUsingGPU = False\n        C.device.try_set_default_device(C.device.cpu())\n\nprint (\"The Cognitive Toolkit is using the {} for processing.\".format(\"GPU\" if isUsingGPU else \"CPU\"))",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": "The Cognitive Toolkit is using the CPU for processing.\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Next, we're going to implement functions to split our data into minibatches. We discussed in Module 3, Lab 2 how minibatches offer a number of advantages over single batch for training.\n\nTo split our data into minibatches, we will use Python's array slice notation. \n\nBy way of a quick explanation of this, the notation works as follows:\n\n    array[start:end] # items start through end-1\n    array[start:]    # items start through the rest of the array\n    array[:end]      # items from the beginning through end-1\n    array[:]         # a copy of the whole array\n\nSo, we'll slice our training data (images in `data_x`, pixel-labels in `data_y`) into `minibatch_size` chunks, where `i` is the chunk number.\n\n**Add your code here to replace the commented section in the cell below** -- your code needs to take `data_x` and slice it into `sx` and `data_y` and slice it into `sy`.  As a hint, for each slice, you'll want to take elements from `i * minibatch_size` (inclusive) to `(i + 1) * minibatch_size` (exclusive), using the Python array slice notation explained above."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def slice_minibatch(data_x, data_y, i, minibatch_size):\n    sx = []\n    sy = []\n    # Use the Python array slice notation the divide up the dataset into minibatches\n    #\n    # *** YOUR CODE HERE ***\n    # Set sx to the slice of data_x from element (i * minibatch_size) to (((i + 1) * minibatch_size)-1)\n    # Set sy to the slice of data_y from element (i * minibatch_size) to (((i + 1) * minibatch_size)-1)\n    # Hint: You'll find answers at the bottom of this lab. \n    # *** YOUR CODE HERE ***\n    sx = data_x[i * minibatch_size: (i+1) * minibatch_size] \n    sy = data_y[i * minibatch_size: (i+1) * minibatch_size] \n\n    return sx, sy",
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Our next function will measure and record the test error per mini-batch, appending it to an errors array. We will use this to calculate the mean error across all mini-batches. \n\n**Again, you will need to fill in some code in the cell below**. Use the Python numpy `mean()` method on the `errors` array to calculate the average error across the minibatch and store the value in the variable named `result`."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def measure_error(data_x_files, data_y_files, x, y, trainer, minibatch_size):\n    errors = []\n    for i in range(0, int(len(data_x_files) / minibatch_size)):\n        data_sx_files, data_sy_files = slice_minibatch(data_x_files, data_y_files, i, minibatch_size)\n        data_sx, data_sy = source.files_to_data(data_sx_files, data_sy_files)\n        errors.append(trainer.test_minibatch({x: data_sx, y: data_sy}))\n\n    result = 0\n    #\n    # *** YOUR CODE HERE ***\n    # Use np.mean to set result to the average of the errors across each minibatch.\n    # Hint: You'll find answers at the bottom of this lab. \n    # *** YOUR CODE HERE ***\n    results = np.mean(errors)\n    \n    return result\n",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Augmenting our ResNet model from Classification to Semantic Segmentation\n\nNow, we need to add additional Fully-Convolutional layers to our ResNet classification model to allow it to perform Semantic Segmentation.\n\nFirst, let's create the functions for our usual ResNet building blocks, as per Module 3 Lab 2:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Built on top of the ResNet model in CNTK repository\n# https://github.com/Microsoft/CNTK/blob/master/Examples/Image/Classification/ResNet/Python/resnet_models.py\n\n#\n# ResNet building blocks\n#\ndef conv_bn(input, filter_size, num_filters, strides=(1,1), init=C.he_normal()):\n    c = Convolution(filter_size, num_filters, activation=None, init=init, pad=True, strides=strides, bias=False)(input)\n    r = BatchNormalization(map_rank=1, normalization_time_constant=4096, use_cntk_engine=False)(c)\n    return r\n\ndef conv_bn_relu(input, filter_size, num_filters, strides=(1,1), init=C.he_normal()):\n    r = conv_bn(input, filter_size, num_filters, strides, init)\n    return relu(r)\n\ndef resnet_basic(input, num_filters):\n    c1 = conv_bn_relu(input, (3,3), num_filters)\n    c2 = conv_bn(c1, (3,3), num_filters)\n    p  = c2 + input\n    return relu(p)\n\ndef resnet_basic_inc(input, num_filters, strides=(2,2)):\n    c1 = conv_bn_relu(input, (3,3), num_filters, strides)\n    c2 = conv_bn(c1, (3,3), num_filters)\n    s  = conv_bn(input, (1,1), num_filters, strides)\n    p  = c2 + s\n    return relu(p)\n\ndef resnet_basic_stack(input, num_stack_layers, num_filters):\n    assert (num_stack_layers >= 0)\n    l = input\n    for _ in range(num_stack_layers):\n        l = resnet_basic(l, num_filters)\n    return l",
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now, we need to add some functions to help us augment our ResNet model with fully-convolutional layers.  \n\nThis is where the magic happens, where we convert ResNet for object classification to ResNet for semantic segmentation. \n\nIn 2015 (https://arxiv.org/pdf/1411.4038.pdf) and 2016 (https://arxiv.org/pdf/1605.06211.pdf), Long, Shelhamer, and Darrell presented a new method for object boundaries detection using Fully Convolutional Networks (FCN). They discovered that fully convolutional networks, trained end-to-end, pixels-to-pixels, exceeded the state-of-the-art in semantic segmentation.\n\nThe main difference from a regular convolutional network is that the final fully-connected classifier is removed and replaced by a combination of all the convolutional features maps extracted from ResNet skipped connections. This allows:\n\n * combining of features from different stages which vary in accuracy of semantic information;\n * upsampling of learned low resolution semantic features maps through learnable one-by-one (1x1) convolutions and 2D upsampling.\n \n \nThe following diagram shows the evolution of our model, from the basic ResNet20 classifier in Module 3, Lab 2, (on the left-hand side) to our fully convolutional network architecture for semantic segmentation in this lab (on the right-hand side):\n![ResNet20 Fully Convolutional Semantic Segmentation Network Architecture](images/resnet20_fcn_semantic_segmentation_network_architecture.svg \"ResNet20 Fully Convolutional Semantic Segmentation Network Architecture\")\n\nIn the right-hand side of the diagram above, the extra convolutional layers coming from the FCN model are actually deconvolutional layers - that is, they are deconvolving back to prediction masks, which are then reshaped and fused by the upsampling layers.\n \nThe function `create_model()` in the following code block shows how this is implemented using the Microsoft Cognitive Toolkit. We take connections from `r1`, `r2_2` and `r3_2` with the feature maps we are interested in to finally combine them in the 'merged' layer after upsampling. A convolution with a sigmoid function classifies the output depending on the number of object classes - in our case, just for airplanes.\n\nThe function `OneByOneConvAndUpSample()`  performs a 1x1 convolution over the input and and upsamples the output using basic Cognitive Toolkit primitives for operating with tensors.\n \nWe also need a dice coefficient function, which is a statistic function commonly used to compare the similarity between two samples. This is what we will use to compare our predicted semantic segmentation labelling against the ground truth for our training and test images."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def UpSampling2D(x):\n    xr = C.reshape(x, (x.shape[0], x.shape[1], 1, x.shape[2], 1))\n    xx = C.splice(xr, xr, axis=-1) # axis=-1 refers to the last axis\n    xy = C.splice(xx, xx, axis=-3) # axis=-3 refers to the middle axis\n    r = C.reshape(xy, (x.shape[0], x.shape[1] * 2, x.shape[2] * 2))\n\n    return r\n\ndef UpSampling2DPower(x, k_power):\n    for i in range(k_power):\n        x = UpSampling2D(x)\n\n    return x\n\ndef OneByOneConvAndUpSample(x, k_power, num_channels):\n    x = Convolution((1, 1), num_channels, init=he_normal(), activation=relu, pad=True)(x)\n    x = UpSampling2DPower(x, k_power)\n\n    return x\n\ndef dice_coefficient(x, y):\n    # average of per-channel dice coefficient\n    # global dice coefificnet doesn't work as class with larger region dominates the metrics\n    # https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient\n    intersection = C.reduce_sum(x * y, axis=(1,2))\n\n    return C.reduce_mean(2.0 * intersection / (C.reduce_sum(x, axis=(1,2)) + C.reduce_sum(y, axis=(1,2)) + 1.0))",
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "With these functions, we can create a new model that is a combination of ResNet plus our additional layers of convolution and upsampling."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#\n# Defines the fully convolutional models for image segmentation\n#\ndef create_model(input, num_classes):\n    c_map = [16, 32, 64]\n    num_stack_layers = 3\n\n    conv = conv_bn_relu(input, (3,3), c_map[0])\n    r1 = resnet_basic_stack(conv, num_stack_layers, c_map[0])\n\n    r2_1 = resnet_basic_inc(r1, c_map[1])\n    r2_2 = resnet_basic_stack(r2_1, num_stack_layers-1, c_map[1])\n\n    r3_1 = resnet_basic_inc(r2_2, c_map[2])\n    r3_2 = resnet_basic_stack(r3_1, num_stack_layers-1, c_map[2])\n    \n    # now we'll add our \"deconvolutional\" layers (1x1 convolution with upsampling)\n\n    up_r1 = OneByOneConvAndUpSample(r1, 0, num_classes)\n    up_r2_2 = OneByOneConvAndUpSample(r2_2, 1, num_classes)\n    up_r3_2 = OneByOneConvAndUpSample(r3_2, 2, num_classes)\n\n    merged = C.splice(up_r1, up_r3_2, up_r2_2, axis=0)\n\n    resnet_fcn_out = Convolution((1, 1), num_classes, init=he_normal(), activation=sigmoid, pad=True)(merged)\n\n    return resnet_fcn_out",
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "As before in Module 3 Lab2, we can look at the Cognitive Toolkit generated view of this model, as rendered by GraphViz:\n\n![ResNet20_Semantic_Segmentation_Model](images/ResNet20_Semantic_Segmentation_Model.svg \"ResNet20 Fully Convolutional Semantic Segmentation Network Model\")\n"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "As this model is generated directly from the Microsoft Cognitive Toolkit, it represents the network that we have actually \nprogrammed the toolkit to construct.  If you are interested, you can download this model from the images/ subdirectory of this lab, and view it separately, comparing it against our reference diagram above."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Training\n\nWe will again create a training function to train our network using mini-batches. \n\nIt is worth repeating that training without a GPU is really slow. Our code here checks whether we are going to be running on a GPU or CPU. If we're running on a CPU, we'll deliberately shrink the amount of training to just a single epoch to give you a *flavor* of what happens during training, but that model will perform pretty badly. In the case of running from a CPU and training for a single epoch, we will read in a pre-trained model for inference instead.\n"
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "def train(train_image_files, train_mask_files, val_image_files, val_mask_files):\n    # Create model\n    sample_img, sample_mask = source.files_to_data([train_image_files[0]], [val_image_files[0]])\n    x = C.input_variable(sample_img[0].shape)\n    y = C.input_variable(sample_mask[0].shape)\n    \n    z = create_model(x, source.num_classes)\n    dice_coef = dice_coefficient(z, y)\n\n    # Prepare model and trainer\n    if (isUsingGPU):\n        lr_mb = [0.001] * 5 + [0.0001] * 5 + [0.00001]*5 + [0.000001]*5 + [0.0000001]*5\n    else:\n        # training without a CPU is really slow, so we'll deliberatly shrink the amount of training\n        # to just an epoch if we're on a CPU - just to give a flavor of what happens during training\n        # and then read in a pre-trained model for inference instead.\n        lr_mb = [0.0001] * 1 # deliberately shrink if training on CPU...\n    lr = learning_rate_schedule(lr_mb, UnitType.sample)\n    momentum = C.learners.momentum_as_time_constant_schedule(0.9)\n    trainer = C.Trainer(z, (-dice_coef, -dice_coef), C.learners.adam(z.parameters, lr=lr, momentum=momentum))\n                        \n    training_errors = []\n    test_errors = []\n\n    # Get minibatches of training data and perform model training\n    minibatch_size = 8\n    num_epochs = len(lr_mb)\n     \n    for e in range(0, num_epochs):\n        for i in tqdm(range(0, int(len(train_image_files) / minibatch_size)), ascii=True, \n                               desc=\"[i] Processing epoch {}/{}\".format(e, num_epochs-1)):\n            data_x_files, data_y_files = slice_minibatch(train_image_files, train_mask_files, i, minibatch_size)\n            data_x, data_y = source.files_to_data(data_x_files, data_y_files)\n            trainer.train_minibatch({x: data_x, y: data_y})\n            gc.collect()\n     \n        # Measure training error\n        training_error = measure_error(data_x_files, data_y_files, x, y, trainer, minibatch_size)\n        training_errors.append(training_error)\n        \n        # Measure test error\n        test_error = measure_error(val_image_files, val_mask_files, x, y, trainer, minibatch_size)\n        test_errors.append(test_error)\n\n        print(\"    epoch #{}: training_error={}, test_error={}\".format(e, training_errors[-1], test_errors[-1]))\n        \n    return trainer, training_errors, test_errors\n",
      "execution_count": 10,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Initialise the Model\n\nNow, let's put it all to the test, and see our solution in action!  First, we need to configure our data source, which involves parsing to figure out how many training and validation samples we have."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Configure the data source\n\nprint('[i] Configuring data source...')\ntry:\n    source = coco.CocoMs(os.path.join(data_path, \"CocoMS\"))\n    training_input_image_files, training_target_mask_files = source.get_data(train_data_folder='/Training')\n    validation_input_image_files, validation_target_mask_files = source.get_data(train_data_folder='/Validation')\n    print('[i] # training samples:   ', len(training_input_image_files))\n    print('[i] # validation samples: ', len(validation_input_image_files))\n    print('[i] # classes:            ', source.num_classes)\n    print('[i] Image size:           ', (224,224))\nexcept (ImportError, AttributeError, RuntimeError) as e:\n    print('[!] Unable to load data source:', str(e))    \n",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[i] reading images and labels...: 100%|##########| 2986/2986 [00:00<00:00, 246267.73it/s]\n[i] reading images and labels...: 100%|##########| 97/97 [00:00<00:00, 35981.91it/s]",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "[i] Configuring data source...\nInitializing CocoMS\n[i] # training samples:    2986\n[i] # validation samples:  97\n[i] # classes:             2\n[i] Image size:            (224, 224)\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "In the next code block, we'll train our model. We'll need to either train from scratch or load a model that we made earlier...\n\n## If training on a CPU\n\nIf we are training on a CPU (like we will be in Azure Notebooks), we deliberately limit the training to a single epoch for time reasons. This gives an example of what training is like, but the resulting model will be very innaccurate. \n\nTry completing the lab using the pre-trained model, and viewing the results. This was trained on a system with a powerful GPU, and run for a number of epochs.\n\nThen restart the Jupyter Notebook and clear all outputs - run to this point again, but set `make_model` to `True` in the code block below below. Continue running on to the end, and you will quickly see how much worse the results are, and get an intuition for how much better the results get with increasing the number of epochs of training.\n\nYou can expect even our very limited training in Azure Notebooks to take 50 minutes or so to complete. By comparison, a GPU could be anywhere from 10x to 100x faster."
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# And now do some training\n\nmake_model = False\n\n# Load the saved model if specified\nmodel_file = \"cntk-resnet-fcn.dnn\"\n\nif (make_model):\n    print(\"[i] Starting training...\", flush=True)\n    if (not isUsingGPU):\n        print(\"    Have you tried running again with make_model set to False to compare output?\", flush=True)\n\n    tic = time.time()\n    trainer, training_errors, test_errors = train(training_input_image_files, training_target_mask_files, \n                                              validation_input_image_files, validation_target_mask_files)\n    print('Training time: {} s.'.format(time.time() - tic))\n\n    print(\"[i] Training finished...\")\n    model = trainer.model  \n    del training_input_image_files\n    del training_target_mask_files\n\nelse: \n    # load model\n    if os.path.isfile(model_file):\n        print(\"[i] Skipping training, using pre-trained model...\")\n        model = C.load_model(model_file)\n    else:\n        print(\"[!] model files {} does not exist. This will force training\".format(model_file))\n        tic = time.time()\n        trainer, training_errors, test_errors = train(training_input_image_files, training_target_mask_files, \n                                                  validation_input_image_files, validation_target_mask_files)\n        print('Training time: {} s.'.format(time.time() - tic))\n        print(\"[i] Training finished...\")\n        model = trainer.model  \n        # clean-up no longer needed variables\n        del training_input_image_files\n        del training_target_mask_files\n",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[i] Skipping training, using pre-trained model...\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## At last, some Semantic Segmentation predictions!\n\nWith our model initialised (either trained or using a downloaded pre-trained model, we can now start making predictions on test images, and try to label them.  We will do this in two steps:\n    \n * First, we will do a forward pass of the input image through our network in order to calculate the actual prediction masks, which contain a class label for each image in the input image;\n * Second, we will create some graphics, allowing us to quickly visualize how well we did."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#\n# We need to convert our validation filenames to image data for the predictor...\n#\n\nprint (\"[i] Converting file lists to image data...\", flush=True)\ntic = time.time()\nvalidation_input_images, validation_target_masks = \\\n        source.files_to_data(validation_input_image_files, validation_target_mask_files)\nprint('Converting validation image data time: {} s.'.format(time.time() - tic))\nprint(\"[i] Converting file lists finished...\")",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[i] Converting file lists to image data...\nConverting validation image data time: 0.9139606952667236 s.\n[i] Converting file lists finished...\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#\n# First, let's do our semantic segmentation prediction\n#\n\nprint(\"[i] Starting prediction...\", flush=True)  \ntic = time.time()\n\npred = []\nfor idx in tqdm(range(0, len(validation_input_images)),ascii=True, desc='[i] Predicting...'):\n    pred += list(model.eval(validation_input_images[idx]))\nprint('Prediction time: {} s.'.format(time.time() - tic))\nprint('[i] {} images predicted.'.format(len(pred)))\nprint(\"[i] Prediction finished...\")\n",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[i] Starting prediction...\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "[i] Predicting...: 100%|##########| 97/97 [00:15<00:00,  5.57it/s]",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "Prediction time: 15.517408847808838 s.\n[i] 97 images predicted.\n[i] Prediction finished...\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "If you have run the Python code above, you'll again see how much faster predicting with a network is compared to training the network.\n\nAnd now some image processing to allow us better understand the results... We are going to blend our input images and predictions together, so we can see where we labeled the pixels correctly and where we did not.\n"
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "#\n# Second, let's do our image processing to create some graphics for ease of visualisation\n#\nprint(\"[i] Started image processing...\", flush=True)\n\ntic = time.time()\n\ninput_images_rgb = []\n\nfor x in tqdm(validation_input_images, ascii=True, desc='[i] Converting input images (BGR2RGB)...'):\n    img = np.moveaxis(x,0,2).astype(np.uint8)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    input_images_rgb.append(img)\n\ntarget_masks_rgb=[]\nfor x in tqdm(validation_target_masks, ascii=True, desc='[i] Coloring ground truth images...'):\n    target_masks_rgb.append(helper.masks_to_colorimg(x))\n\npred_rgb=[]\nfor x in tqdm(pred, ascii=True, desc='[i] Coloring prediction images...'):\n    pred_rgb.append(helper.masks_to_colorimg(x))\n\noutput_images_rgb = []\nfor index in tqdm(range(len(input_images_rgb)), ascii=True, desc='[i] Combining input images + predictions...'):\n    img = cv2.bitwise_or(input_images_rgb[index], pred_rgb[index])\n    output_images_rgb.append(img)\n\nprint('Image Processing time: {} s.'.format(time.time() - tic))\n\nprint(\"Image processing finished...\")\n",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[i] Started image processing...\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "[i] Converting input images (BGR2RGB)...: 100%|##########| 97/97 [00:00<00:00, 794.56it/s]\n[i] Coloring ground truth images...: 100%|##########| 97/97 [00:00<00:00, 378.28it/s]\n[i] Coloring prediction images...: 100%|##########| 97/97 [00:00<00:00, 300.01it/s]\n[i] Combining input images + predictions...: 100%|##########| 97/97 [00:00<00:00, 4543.19it/s]",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "Image Processing time: 0.7506537437438965 s.\nImage processing finished...\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "If we have trained, we can view the learning curves for our training and our testing. All going well, these will have converged to a reasonable error rate, based on our dice coefficient. Ideally, we want the curves for test and training sets to be as close together as possible. Normally we'll see the curve for the training set is slightly more accurate (lower `dice_coef` value) than the test set.\n\nIf you have trained locally on a GPU, they should look something like:\n\n![sample training curve](images/sample_training_curve.png \"Sample Training Curve\")\n\nIf you have trained on a CPU and only for one epoch, you'll only have one reading for test and training error, so you might get a funny graph like:\n\n![cpu limited_training curve](images/cpu_limited_training_curve.png \"CPU Limited Training Curve\")\n\nIf you have trained on CPU and see that type of curve, don't worry about it - it makes perfect sense for the limited (but costly in terms of time) amount of training you performed!  Your network has not converged *yet* and it would not be expected perform well. If you had plenty of patience, you could continue to train it for longer and see the curves converging.\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "if (make_model):\n    helper.plot_errors({\"training\": training_errors, \"test\": test_errors}, title=\"Simulation Learning Curve\")\n                \n    # clean-up some variables we no longer need to reduce our memory footprint...\n    # otherwise our Azure Notebook might run out of memory\n    del validation_input_images\n    del validation_input_image_files\n    del validation_target_mask_files\n    del validation_target_masks\n    del trainer\n    del training_errors\n    del test_errors\n    print(\"Garbage collection reclaimed {} objects\".format(gc.collect()))\n    plt.close()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## *A picture is worth a thousand words*\n\nAnd now, some images. We'll plot an original test image, with its associated ground truth mask/label. Alongside these, we'll show our predicted mask, and finally our predicted mask super-imposed on the original image, allowing us to visually see how well our model did. **Note: this plot function takes a while!**"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "helper.plot_side_by_side([input_images_rgb, target_masks_rgb, pred_rgb, output_images_rgb])",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": "100%|##########| 388/388 [00:27<00:00, 14.20it/s]\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "If you took the pre-trained model or trained via a GPU, we did well, considering the network learned *from scratch* how to label planes of all sorts, and from all viewpoints and angles. However, we can see that in some cases the network did make mistakes. This is mostly due to our network being small and our training set being very small -- to make this lab run in a reasonable amount of time.\n\nIf you trained on a CPU, the very limited training will mean we're just barely detecting the planes and our model has a lot more *learning* to do to be useful.\n\nA larger network would be more expressive (e.g., ResNet-150), and could capture greater detail. If running on a GPU, we could relatively easily switch in a larger/deeper ResNet model and benefit from its greater capabilities.\n\nAdditionally, a larger corpus of training examples would allow the network to better learn the objective, and to generalise much more accurately to new examples.\n\nThere is something we can do to help tremendously with the limited training set by making use of *Transfer Learning*.  We will see in in the next lab.\n\nFor now, let's take a look at some of the more interesting samples."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "plt.imshow(output_images_rgb[89])\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "plt.imshow(output_images_rgb[2])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "plt.imshow(output_images_rgb[6])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "plt.imshow(output_images_rgb[7])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "plt.imshow(output_images_rgb[8])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "plt.imshow(output_images_rgb[11])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Conclusions \n\nIn this lab, we used the Microsoft Cognitive Toolkit to build a ResNet20 model and modified it to perform semantic segmentation.\n\nWe used the Coco dataset, which provides labels and masks for a variety of object classes. We focused on a subset containing airplanes.  We either used a pre-trained model or used this subset to train our model using the training data in mini-batches. Then we used this trained model to perform segmentation on some test images and plotted the results.\n\nDespite the fact that our accuracy was limited by the size of the base model (ResNet20, vs its \nlarger cousin ResNet150, for example) and the size of the training set (~3K images), we demonstrated \nsegmentation results far superior than could be expected of the traditional hand-crafted methods we saw in Module 2.\n\nIn the next lab, we'll look at transfer learning -- a way of bringing in extra smarts from another\nmodel already trained on a much larger but different dataset. These extra smarts can help boost the\nperformance of our semantic segmentation considerably."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**Solution to missing slice_minibatch code**\n\n    sx = data_x[i * minibatch_size: (i+1) * minibatch_size] \n    sy = data_y[i * minibatch_size: (i+1) * minibatch_size] \n"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**Solution to missing measure_error code**\n\n    result = np.mean(errors)\n\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}