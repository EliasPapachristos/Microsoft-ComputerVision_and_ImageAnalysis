{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Module 4, Lab 2: Using ResNet with Fully Convolutional Layers and Transfer Learning to Implement Semantic Segmentation\n\nIn this tutorial, we will add fully-convolutional layers to the ResNet model using transfer learning.\n\nWe will introduce and use the concept of *Transfer Learning*, where pre-existing learned knowledge is used to inform our approach and improve our results.\n\nLet's get started!\n\nWe built up a lot of code in the last tutorial, so we won't replicate it.  Instead, we're going to import Python files from the directory of this lab that implements the same functionality. Feel free to view that code to inspect it!\n\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Copyright (c) Microsoft. All rights reserved.\n#\n# Licensed under the MIT license. See LICENSE.md file in the project root\n# for full license information.\n# ==============================================================================\n\n# For Azure Notebooks, we will update Microsoft Cognitive Toolkit version to 2.4 \n# you can comment out the following line if you are running in your own local Jupyter Notebook setup and already have\n# CNTK 2.4 installed\n!pip install --upgrade --no-deps https://cntk.ai/PythonWheel/CPU-Only/cntk-2.4-cp35-cp35m-linux_x86_64.whl\n\nimport cntk as C\nprint (\"Using Microsoft Cognitive Toolkit version {}\".format(C.__version__))\n\nimport numpy as np\nprint (\"Using numpy version {}\".format(np.__version__))",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Collecting cntk==2.4 from https://cntk.ai/PythonWheel/CPU-Only/cntk-2.4-cp35-cp35m-linux_x86_64.whl\n  Using cached https://cntk.ai/PythonWheel/CPU-Only/cntk-2.4-cp35-cp35m-linux_x86_64.whl\nInstalling collected packages: cntk\n  Found existing installation: cntk 2.4\n    Uninstalling cntk-2.4:\n      Successfully uninstalled cntk-2.4\nSuccessfully installed cntk-2.4\nUsing Microsoft Cognitive Toolkit version 2.4\nUsing numpy version 1.15.4\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import time\nimport os\nimport cv2\nimport gc\n\nfrom cntk.learners import learning_rate_schedule, UnitType\nfrom cntk.device import try_set_default_device, gpu\nfrom tqdm import tqdm\nfrom cntk.initializer import he_normal\nfrom cntk.layers import AveragePooling, BatchNormalization, Convolution, Dense\nfrom cntk.ops import element_times, relu, sigmoid\nfrom cntk import load_model, placeholder, Constant\n\nimport coco   # local class to read the COCO images and labels\nimport helper # some functions to plot images\nimport training_helper  #\nimport cntk_resnet_fcn\nimport matplotlib.pyplot as plt\n\n# Paths relative to current python file.\nabs_path  = os.path.dirname(os.path.abspath(\".\"))\ndata_path = os.path.join(abs_path, \"../../data/M4\")\nzip_path = os.path.join(abs_path, \"../data-zip\")\n\nmodel_path = os.path.join(abs_path, \"Lab2/models\")\nbase_model_file = os.path.join(model_path, \"ResNet18_ImageNet_CNTK.model\")\n",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We are going to use the Cognitive Toolkit's default policy to use the best available device (GPU, if available, else CPU)."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "try:\n    isUsingGPU = C.device.try_set_default_device(C.device.gpu(0))\nexcept ValueError:\n    isUsingGPU = False\n    C.device.try_set_default_device(C.device.cpu())\n    \nprint (\"[i] The Cognitive Toolkit is using the {} for processing\".format(\"GPU\" if isUsingGPU else \"CPU\"))\n",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[i] The Cognitive Toolkit is using the CPU for processing\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Finally, if you are interested in running training (and have a GPU, or a CPU with lots of patience), set `make_model` to true and run the training code below.\n\nOtherwise, set this to false, and download the already-baked pre-trained model directly from Microsoft."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "make_model = False",
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Transfer Learning\n\nNowadays, few convolutionl image networks are trained from scratch with purely random initialization.  Mostly, this is because of the difficulty in finding a suitably large, labelled, dataset.  If the target dataset is very small, it is likely that training with it would lead to unacceptably large generalization error.\n\nInstead, the most common approach is to *pre-train* using a more generic large dataset (for example, ImageNet, or CoCo) and then use the learned features from this set as an initial seed to continue training with the target smaller dataset.  \n\nThis is using the intuition that the knowledge gained (within the layers of the network) while learning to recognize certain objects could be useful when trying to recognize objects of a different type. In other words, the network will learn more useful and generic *features* in its layers when trained against a larger dataset, which it can then apply (*transfer*) to the dataset of interest.\n\nThis approach is called *Transfer Learning*. \n\n![Transfer Learning](images/Transfer_Learning.png \"Transfer Learning\")\n\n## Considerations when using Transfer Learning\n\nIf the target dataset is sigificantly smaller in size but of similar content to the original dataset, it is not always a good idea to fine-tune as it may lead to overfitting - since the data is similar to the original data, already learned features are very likely to be relevant (\"transferrable\") to this dataset as well, and already generalized from the content in the larger original dataset.\n\nIf the target dataset is relatively large in size and again of similar content to the original dataset, we can try fine-tuning only as we are less likely to overfit.\n\nIf our target dataset is smaller in size but dissimilar in content to the original dataset, it is likely that the later domain-specific layers in the original network have learned features that are not relevant to our target network. In this case, it might work better to remove some of these later layers, and add any new domain-specific layers to  activations from earlier in the original network.\n\nIf our target dataset is large and very dissimilar in content to the original dataset, we could train from scratch, but in practice it is often useful to initialize with weights taken from a pre-trained model. However, we don't need to freeze any of the layers and would typically just use our original dataset as a starting point.\n\nAdditionally, we will tend to transition from having general (*well-transferrable*) features early in the network, to more specific (*less-transferrable*) features in later layers of the network. This is called *domain discrepancy*. More advanced strategies might take account of this, and could include usung different learning rates by layer - that is, adjusting the learning weight of a layer (determining how *slushy* to make it, from frozen to fully tweakable) depending on its depth in the network.\n\n![Transfer Learning Strategies](images/Transfer_Learning_Strategies.png \"Transfer Learning Strategies\")\n\n\n## Our Examples\n\nIn our examples,  we are going to take a pre-trained set of weights for an object classifier network (ResNet18, trained on ImageNet), and try two different approaches:\n * In the first approach, we will *freeze* these weights, and only train additional layers to perform semantic segmentation.\n * In the second approach, we will not freeze the original weights, but *fine-tune* them by continuing to train and allowing backpropagation to alter the weights in the earlier layers of the network.\n \nNote that this is not quite the same model that we used in Module 4, Lab 1, as we're using a small model that is pre-trained. As a result, our segmentation accuracy will be more *blocky*, as we are upsampling from smaller layers. But this is an artefact of engineering the lab to run in a limited computation environment, and to keep training size down. In general, transfer learning will offer better accuracy for limited training sets, and quite often faster training as well (if freezing layers).\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "feature_node_name = \"features\"\nlast_hidden_node_name = \"z.x\"\nimage_height = 224\nimage_width = 224\nnum_channels = 3\n\n#\n# Defines the fully convolutional models for image segmentation (transfer learning)\n#\ndef create_transfer_learning_model(input, num_classes, model_file, freeze=False):\n\n    base_model = load_model(model_file)\n    base_model = C.as_composite(base_model[3].owner)\n\n    # Load the pretrained classification net and find nodes\n    feature_node = C.logging.find_by_name(base_model, feature_node_name)\n    last_node = C.logging.find_by_name(base_model, last_hidden_node_name)\n    \n    base_model = C.combine([last_node.owner]).clone(C.CloneMethod.freeze if freeze else C.CloneMethod.clone, {feature_node: C.placeholder(name='features')})\n    base_model = base_model(C.input_variable((num_channels, image_height, image_width)))\n\n    r1 = C.logging.find_by_name(base_model, \"z.x.x.r\")\n    r2_2 = C.logging.find_by_name(base_model, \"z.x.x.x.x.r\")\n    r3_2 = C.logging.find_by_name(base_model, \"z.x.x.x.x.x.x.r\")\n\n    up_r1 = cntk_resnet_fcn.OneByOneConvAndUpSample(r1, 3, num_classes)\n    up_r2_2 = cntk_resnet_fcn.OneByOneConvAndUpSample(r2_2, 2, num_classes)\n    up_r3_2 = cntk_resnet_fcn.OneByOneConvAndUpSample(r3_2, 1, num_classes)\n    \n    merged = C.splice(up_r1, up_r2_2, up_r3_2, axis=0)\n\n    resnet_fcn_out = Convolution((1, 1), num_classes, init=he_normal(), activation=sigmoid, pad=True)(merged)\n\n    z = cntk_resnet_fcn.UpSampling2DPower(resnet_fcn_out,2)\n    \n    return z\n",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now, let's setup and call our function to setup the large dataset that is  needed for this lab. \n\nWhy are we doing this? We need this step because Azure Notebooks has two storage areas: a persistent but slow area, and a transient but fast area. To ensure that this lab is able to execute as fast as possible on Azure Notebooks, we unzip our data set into this transient area once per session.  If it has already been unzipped then we'll automatically detect this and skip the step, so it is safe to run this next code block at any time.\n\nDepending on how you are executing the lab, this step can take a minute or two."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import sys\nimport os\nimport zipfile\nimport fnmatch\n\ndef hydrate(zip_path, dest_path):\n    print(\"Start unzipping data files in {0} to {1}\".format(zip_path,dest_path))\n    if (os.path.exists(zip_path) == False):\n        print(\"The source folder {0} doesn't exist, so quitting\".format(zip_path))\n        quit()\n\n    zipfile_count = len(fnmatch.filter(os.listdir(zip_path), '*.zip'))\n    if (zipfile_count == 0):\n        print(\"No zip (.zip) files in {0}, so quitting \".format(zip_path))\n\n    print(\"zip file count:%s\" % zipfile_count)\n\n    if (os.path.exists(dest_path) == False):\n        print(\"Destination folder {0} doesn't exist, creating it\".format(dest_path))\n        os.makedirs(dest_path)\n\n        # Extract all zip files from zip_path to dest_path\n        print(\"Start unzipping files to {0}\".format(dest_path))\n        for item in os.listdir(zip_path): # loop through items in dir\n            if item.endswith(\".zip\"): # check for \".zip\" extension\n                print(\"   unzipping {0} ...\".format(item))\n                file_name = os.path.join(zip_path,item) # get full path of files\n                zip_ref = zipfile.ZipFile(file_name) # create zipfile object\n                zip_ref.extractall(dest_path) # extract file to dir\n                zip_ref.close() # close file\n    else:\n        print(\"data folder already populated\")\n\n    print(\"Complete: Files have been unzipped to {0}\".format(dest_path))\n    \nhydrate(zip_path, data_path)",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Start unzipping data files in /home/nbuser/library/Module4/../data-zip to /home/nbuser/library/Module4/../../data/M4\nzip file count:6\ndata folder already populated\nComplete: Files have been unzipped to /home/nbuser/library/Module4/../../data/M4\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We need to load our dataset."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Configure the data source\n\n    \nprint('[i] Configuring data source...')\ntry:\n    source = coco.CocoMs(os.path.join(data_path, \"CocoMS\"))\n    training_input_image_files, training_target_mask_files = source.get_data(train_data_folder='/Training')\n    validation_input_image_files, validation_target_mask_files = source.get_data(train_data_folder='/Validation')\n    print('[i] # training samples:   ', len(training_input_image_files))\n    print('[i] # validation samples: ', len(validation_input_image_files))\n    print('[i] # classes:            ', source.num_classes)\n    print('[i] Image size:           ', (224,224))\nexcept (ImportError, AttributeError, RuntimeError) as e:\n    print('[!] Unable to load data source:', str(e))    \n",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[i] reading images and labels...: 100%|##########| 2986/2986 [00:00<00:00, 152991.52it/s]",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "[i] Configuring data source...\nInitializing CocoMS\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "\n[i] reading images and labels...: 100%|##########| 97/97 [00:00<00:00, 52159.93it/s]",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "[i] # training samples:    2986\n[i] # validation samples:  97\n[i] # classes:             2\n[i] Image size:            (224, 224)\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We are going to encapsulate our image processing routines from the last lab into a handy function for re-use.\n\nThis creates some images to visualize how well our semantic segmenter worked out against our test set."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Drawing\n\ndef process_images():\n    print(\"[i] Started image processing...\", flush=True)\n    tic = time.time()\n    \n    input_images_rgb = []\n    for x in tqdm(validation_input_images, ascii=True, desc='[i] Converting input images (BGR2RGB)...'):\n        img = np.moveaxis(x,0,2).astype(np.uint8)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        input_images_rgb.append(img)\n\n    target_masks_rgb=[]\n    for x in tqdm(validation_target_masks, ascii=True, desc='[i] Coloring ground truth images...'):\n        target_masks_rgb.append(helper.masks_to_colorimg(x))\n\n    pred_rgb=[]\n    for x in tqdm(pred, ascii=True, desc='[i] Coloring prediction images...'):\n        pred_rgb.append(helper.masks_to_colorimg(x))\n\n    output_images_rgb = []\n    for index in tqdm(range(len(input_images_rgb)), ascii=True, desc='[i] Combining input images + predictions...'):\n        img = cv2.bitwise_or(input_images_rgb[index], pred_rgb[index])\n        output_images_rgb.append(img)\n\n    print('Image Processing time: {} s.'.format(time.time() - tic))\n    print(\"Image processing finished... Now plotting (this can take a while - 2-3 minutes on Azure Notebooks)...\", flush=True)\n    helper.plot_side_by_side([input_images_rgb, target_masks_rgb, pred_rgb, output_images_rgb])",
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Our Trainer\n\nThis time, our training function was create transfer learning models for the network."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def train(train_image_files, train_mask_files, val_image_files, val_mask_files, base_model_file, freeze=False):\n    # Create model\n    sample_img, sample_mask = source.files_to_data([train_image_files[0]], [val_image_files[0]])\n    x = C.input_variable(sample_img[0].shape)\n    y = C.input_variable(sample_mask[0].shape)\n    \n    z = create_transfer_learning_model(x, source.num_classes, base_model_file, freeze)\n    dice_coef = cntk_resnet_fcn.dice_coefficient(z, y)\n\n\n    # Prepare model and trainer\n    if (isUsingGPU):\n        lr_mb = [0.001] * 5 + [0.0001] * 5 + [0.00001]*5 + [0.000001]*5 + [0.0000001]*5\n    else:\n        # training without a CPU is really slow, so we'll deliberatly shrink the amount of training\n        # to just an epoch if we're on a CPU - just to give a flavor of what happens during training\n        # and then read in a pre-trained model for inference instead.\n        lr_mb = [0.0001] * 1 # deliberately shrink if training on CPU...\n    lr = learning_rate_schedule(lr_mb, UnitType.sample)\n    momentum = C.learners.momentum_as_time_constant_schedule(0.9)\n    trainer = C.Trainer(z, (-dice_coef, -dice_coef), C.learners.adam(z.parameters, lr=lr, momentum=momentum))\n                        \n    training_errors = []\n    test_errors = []\n\n    # Get minibatches of training data and perform model training\n    minibatch_size = 8\n    num_epochs = len(lr_mb)\n     \n    for e in range(0, num_epochs):\n        for i in tqdm(range(0, int(len(train_image_files) / minibatch_size)), ascii=True, \n                               desc=\"[i] Processing epoch {}/{}\".format(e, num_epochs-1)):\n            data_x_files, data_y_files = training_helper.slice_minibatch(train_image_files, train_mask_files, i, minibatch_size)\n            data_x, data_y = source.files_to_data(data_x_files, data_y_files)\n            trainer.train_minibatch({z.arguments[0]: data_x, y: data_y})\n            gc.collect()\n     \n        # Measure training error\n        training_error = training_helper.measure_error(source, data_x_files, data_y_files, z.arguments[0], y, trainer, minibatch_size)\n        training_errors.append(training_error)\n        \n        # Measure test error\n        test_error = training_helper.measure_error(source, val_image_files, val_mask_files, z.arguments[0], y, trainer, minibatch_size)\n        test_errors.append(test_error)\n\n        print(\"epoch #{}: training_error={}, test_error={}\".format(e, training_errors[-1], test_errors[-1]))\n        \n    return trainer, training_errors, test_errors",
      "execution_count": 10,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Freezing\n\nAnd now time to start training.  In the first approach, we will *freeze* these weights, and only train additional layers to perform semantic segmentation.\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#\n# We need to convert our validation filenames to image data for the predictor...\n#\n\nprint (\"[i] Converting file lists to image data...\", flush=True)\ntic = time.time()\nvalidation_input_images, validation_target_masks = \\\n        source.files_to_data(validation_input_image_files, validation_target_mask_files)\nprint('Converting validation image data time: {} s.'.format(time.time() - tic))\nprint(\"[i] Converting file lists finished...\")",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[i] Converting file lists to image data...\nConverting validation image data time: 1.4227368831634521 s.\n[i] Converting file lists finished...\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Training with original layer weights frozen)\n\nif make_model:\n    print(\"[i] Starting training (with frozen weights)...\", flush=True)\n    frozen = True\n    tic = time.time()\n    trainer, training_errors, test_errors = train(training_input_image_files, training_target_mask_files,  \n                                                  validation_input_image_files, validation_target_mask_files, \n                                                  base_model_file, frozen)\n    print('Training time: {}'.format(time.time() - tic))\n    print(\"[i] Training finished...\")\n    model = trainer.model\nelse:\n    print(\"[i] Skipping training, using pre-trained model...\")\n    model = C.load_model(os.path.join(model_path, 'cntk-resnet-fcn-transfer-frozen.dnn'))\n\n# Prediction\n\nprint(\"[i] Starting prediction...\", flush=True)\n\npred = []\nfor idx in tqdm(range(0, len(validation_input_images)),ascii=True, desc='[i] Predicting...'):\n    pred += list(model.eval(validation_input_images[idx]))\n\nprint('[i] {} images predicted.'.format(len(pred)))\nprint(\"[i] Prediction finished...\")\n\nif make_model:\n    helper.plot_errors({\"training\": training_errors, \"test\": test_errors}, title=\"Simulation Learning Curve\")",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[i] Skipping training, using pre-trained model...\n[i] Starting prediction...\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "[i] Predicting...: 100%|##########| 97/97 [00:13<00:00,  8.52it/s]",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "[i] 97 images predicted.\n[i] Prediction finished...\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "process_images()\nprint(\"Garbage collection reclaimed {} objects\".format(gc.collect()))",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[i] Started image processing...\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "[i] Converting input images (BGR2RGB)...: 100%|##########| 97/97 [00:00<00:00, 976.36it/s]\n[i] Coloring ground truth images...: 100%|##########| 97/97 [00:00<00:00, 359.58it/s]\n[i] Coloring prediction images...: 100%|##########| 97/97 [00:00<00:00, 350.12it/s]\n[i] Combining input images + predictions...: 100%|##########| 97/97 [00:00<00:00, 5645.01it/s]",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "Image Processing time: 0.6885623931884766 s.\nImage processing finished... Now plotting (this can take a while - 2-3 minutes on Azure Notebooks)...\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "\n100%|##########| 388/388 [00:31<00:00, 12.45it/s]\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "Garbage collection reclaimed 0 objects\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Did you notice how much faster training is with transfer learning and frozen initial layers versus the full model training we did in Module 4 Lab 1? We're just training a few additional layers to implement Semantic Segmentation based on a base model of ResNet...\n\nFor the frozen layers, we don't have to retrain them -- their weights are frozen. What this means is that we don't have to run back prop on them to calculate error gradients and update weights. We just do forward prop through those layers and back prop only on the final additional layers to train their weights."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Fine-Tuning\n\nIn the second approach, we will not freeze the original weights, but *fine-tune* them by continuing to train and allowing backpropagation to alter the weights in the earlier layers of the network."
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Training with Fine-Tuning\n\nif make_model:\n    print(\"[i] Starting training (with fine-tuning)...\", flush=True)\n    frozen = False\n    tic = time.time()\n    trainer, training_errors, test_errors = train(training_input_image_files, training_target_mask_files, \n                                                  validation_input_image_files, validation_target_mask_files, \n                                                  base_model_file, frozen)\n    print('Training time: {} s.'.format(time.time() - tic))\n    print(\"[i] Training finished...\")\n    model = trainer.model\nelse:\n    model = C.load_model(os.path.join(model_path, 'cntk-resnet-fcn-transfer-finetune.dnn'))\n\n# Prediction\n\nprint(\"[i] Starting prediction...\", flush=True)\n\npred = []\nfor idx in tqdm(range(0, len(validation_input_images)),ascii=True, desc='[i] Predicting...'):\n    pred += list(model.eval(validation_input_images[idx]))\n\nprint('[i] {} images predicted.'.format(len(pred)))\nprint(\"[i] Prediction finished...\")\n\nif make_model:\n    helper.plot_errors({\"training\": training_errors, \"test\": test_errors}, title=\"Simulation Learning Curve\")\n    # clean-up some variables we no longer need to reduce our memory footprint...\n    # otherwise our Azure Notebook might run out of memory\n    del trainer\n    del training_errors\n    del test_errors",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "process_images()\nprint(\"Garbage collection reclaimed {} objects\".format(gc.collect()))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Conclusions\n\nIn this lab, we added fully-convolutional layers to a base ResNet model and use the concept of Transfer Learning to train our models.\n\nTransfer Learning is the defacto means of training models nowadays for new data sets. It is a technique that allows a network to learn a new skill (recognition of a new class of object, for instance) by employing the knowledge it has already learned about similar types of skills (i.e., an ability to detect edges, corners, and more complex shapes, built up in its various layers through learning to recognise difference classes of objects with a large data set).\n\nWe too a pre-trained set of weights for an object classifier network (ResNet, trained on ImageNet), and tried two different approaches - the first where we froze the base model weights, and only allowed the new layers to learn, and the second where we seeded the initial layers with weights from the base model, but allowed all layers in the model to update and learn a solution to our problem."
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}